[{"content":"## The story behind the Airflow Breeze tool\nInitially, we started contributing to this fantastic open-source project [Apache Airflow] with a team of three which then grew to five. When we kicked it off a year ago, I realized pretty soon where the biggest bottlenecks and areas for improvement in terms of productivity were. Even with the help of our client, who provided us with a “homegrown” development environment it took us literally days to set it up and learn some basics.\n\nThat is how the journey to increased productivity in Apache Airflow began. The result? The Airflow Breeze open-source tool. Jarek Potiuk, an Airflow Committer, will tell you all about it.\n\nYou can learn [how and why it’s a \"Breeze\" to Develop Apache Airflow](https://higrys.medium.com/its-a-breeze-to-develop-apache-airflow-bf306d3e3505).\n","url":"Its-a-breeze-to-develop-apache-airflow","title":"It's a \"Breeze\" to develop Apache Airflow","linkTitle":"It's a \"Breeze\" to develop Apache Airflow","author":"Jarek Potiuk","twitter":"higrys","github":"potiuk","linkedin":"jarekpotiuk","description":"A Principal Software Engineer's journey to developer productivity. Learn how Jarek and his team speeded up and simplified Airflow development for the community.","tags":["Development"],"date":"2019-11-22T00:00:00.000Z"},{"content":"","url":"_index","title":"Blog","linkTitle":"Blog","menu":{"main":{"weight":25}}},{"content":"Airflow 1.10.10 contains 199 commits since 1.10.9 and includes 11 new features, 43 improvements, 44 bug fixes, and several doc changes.\n\n**Details**:\n\n* **PyPI**: [https://pypi.org/project/apache-airflow/1.10.10/](https://pypi.org/project/apache-airflow/1.10.10/)\n* **Docs**: [https://airflow.apache.org/docs/1.10.10/](https://airflow.apache.org/docs/1.10.10/)\n* **Changelog**: [http://airflow.apache.org/docs/1.10.10/changelog.html](http://airflow.apache.org/docs/1.10.10/changelog.html)\n\nSome of the noteworthy new features (user-facing) are:\n\n- [Allow user to chose timezone to use in the RBAC UI](https://github.com/apache/airflow/pull/8046)\n- [Add Production Docker image support](https://github.com/apache/airflow/pull/7832)\n- [Allow Retrieving Airflow Connections & Variables from various Secrets backend](http://airflow.apache.org/docs/1.10.10/howto/use-alternative-secrets-backend.html)\n- [Stateless Webserver using DAG Serialization](http://airflow.apache.org/docs/1.10.10/dag-serialization.html)\n- [Tasks with Dummy Operators are no longer sent to executor](https://github.com/apache/airflow/pull/7880)\n- [Allow passing DagRun conf when triggering dags via UI](https://github.com/apache/airflow/pull/7312)\n\n### Allow user to chose timezone to use in the RBAC UI\n\nBy default the Web UI will show times in UTC. It is possible to change the timezone shown by using the menu in the top\n right (click on the clock to activate it):\n\n**Screenshot**:\n![Allow user to chose timezone to use in the RBAC UI](rbac-ui-timezone.gif)\n\nDetails: https://airflow.apache.org/docs/1.10.10/timezone.html#web-ui\n\n**Note**: This feature is only available for the RBAC UI (enabled using `rbac=True` in `[webserver]` section in your `airflow.cfg`).\n\n### Add Production Docker image support\n\nThere are brand new production images (alpha quality) available for Airflow 1.10.10. You can pull them from the\n[Apache Airflow Dockerhub](https://hub.docker.com/r/apache/airflow) repository and start using it.\n\nMore information about using production images can be found in https://github.com/apache/airflow/blob/master/IMAGES.rst#using-the-images. Soon it will be updated with\ninformation how to use images using official helm chart.\n\nTo pull the images you can run one of the following commands:\n\n- `docker pull apache/airflow:1.10.10-python2.7`\n- `docker pull apache/airflow:1.10.10-python3.5`\n- `docker pull apache/airflow:1.10.10-python3.6`\n- `docker pull apache/airflow:1.10.10-python3.7`\n- `docker pull apache/airflow:1.10.10` (uses Python 3.6)\n\n### Allow Retrieving Airflow Connections & Variables from various Secrets backend\n\nFrom Airflow 1.10.10, users would be able to get Airflow Variables from Environment Variables.\n\nDetails: https://airflow.apache.org/docs/1.10.10/concepts.html#storing-variables-in-environment-variables\n\nA new concept of Secrets Backend has been introduced to retrieve Airflow Connections and Variables.\n\nFrom Airflow 1.10.10, users can retrieve Connections & Variables using the same sy","url":"airflow-1.10","title":"Apache Airflow 1.10.10","linkTitle":"Apache Airflow 1.10.10","author":"Kaxil Naik","twitter":"kaxil","github":"kaxil","linkedin":"kaxil","description":"We are happy to present Apache Airflow 1.10.10","tags":["release"],"date":"2020-04-09T00:00:00.000Z"},{"content":"Airflow 1.10.12 contains 113 commits since 1.10.11 and includes 5 new features, 23 improvements, 23 bug fixes,\nand several doc changes.\n\n**Details**:\n\n* **PyPI**: [https://pypi.org/project/apache-airflow/1.10.12/](https://pypi.org/project/apache-airflow/1.10.12/)\n* **Docs**: [https://airflow.apache.org/docs/1.10.12/](https://airflow.apache.org/docs/1.10.12/)\n* **Changelog**: [http://airflow.apache.org/docs/1.10.12/changelog.html](http://airflow.apache.org/docs/1.10.12/changelog.html)\n\n\n**Airflow 1.10.11 has breaking changes with respect to\nKubernetesExecutor & KubernetesPodOperator so I recommend users to directly upgrade to Airflow 1.10.12 instead**.\n\nSome of the noteworthy new features (user-facing) are:\n\n- [Allow defining custom XCom class](https://github.com/apache/airflow/pull/8560)\n- [Get Airflow configs with sensitive data from Secret Backends](https://github.com/apache/airflow/pull/9645)\n- [Add AirflowClusterPolicyViolation support to Airflow local settings](https://github.com/apache/airflow/pull/10282)\n\n### Allow defining Custom XCom class\n\nUntil Airflow 1.10.11, the XCom data was only stored in Airflow Metadatabase. From Airflow 1.10.12, users\nwould be able to define custom XCom classes. This will allow users to transfer larger data between tasks.\nAn example here would be to store XCom in S3 or GCS Bucket if the size of data that needs to be stored is larger\nthan `XCom.MAX_XCOM_SIZE` (48 KB).\n\n**PR**: https://github.com/apache/airflow/pull/8560\n\n### Get Airflow configs with sensitive data from Secret Backends\n\nUsers would be able to get the following Airflow configs from Secrets Backend like Hashicorp Vault:\n\n   - `sql_alchemy_conn` in [core] section\n   - `fernet_key` in [core] section\n   - `broker_url` in [celery] section\n   - `flower_basic_auth` in [celery] section\n   - `result_backend` in [celery] section\n   - `password` in [atlas] section\n   - `smtp_password` in [smtp] section\n   - `bind_password` in [ldap] section\n   - `git_password` in [kubernetes] section\n\nFurther improving Airflow's Secret Management story, from Airflow 1.10.12, users don't need to hardcode\nthe **sensitive** config value in airflow.cfg nor then need to use an Environment variable to set this config.\n\nFor example, the metadata database connection string can either be set in airflow.cfg like this:\n\n```ini\n[core]\nsql_alchemy_conn_secret = sql_alchemy_conn\n```\nThis will retrieve config option from the set Secret Backends.\n\nAs you can see you just need to add a `_secret` suffix at the end of the actual config option\nand the value needs to be the **key** which the Secrets backend will look for.\n\nSimilarly, `_secret` config options can also be set using a corresponding environment variable. For example:\n\n```\nexport AIRFLOW__CORE__SQL_ALCHEMY_CONN_SECRET=sql_alchemy_conn\n```\n\nMore details: http://airflow.apache.org/docs/1.10.12/howto/set-config.html\n\n### Add AirflowClusterPolicyViolation support to airflow_local_settings.py\n\nUsers can use Cluster Policies to apply clust","url":"airflow-1.10","title":"Apache Airflow 1.10.12","linkTitle":"Apache Airflow 1.10.12","author":"Kaxil Naik","twitter":"kaxil","github":"kaxil","linkedin":"kaxil","description":"We are happy to present Apache Airflow 1.10.12","tags":["release"],"date":"2020-08-25T00:00:00.000Z"},{"content":"Airflow 1.10.8 contains 160 commits since 1.10.7 and includes 4 new features, 42 improvements, 36 bug fixes, and several doc changes.\n\nWe released 1.10.9 on the same day as one of the Flask dependencies (Werkzeug) released 1.0 which broke Airflow 1.10.8.\n\n**Details**:\n\n* **PyPI**: [https://pypi.org/project/apache-airflow/1.10.9/](https://pypi.org/project/apache-airflow/1.10.9/)\n* **Docs**: [https://airflow.apache.org/docs/1.10.9/](https://airflow.apache.org/docs/1.10.9/)\n* **Changelog (1.10.8)**: [http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07](http://airflow.apache.org/docs/1.10.8/changelog.html#airflow-1-10-8-2020-01-07)\n* **Changelog (1.10.9)**: [http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10](http://airflow.apache.org/docs/1.10.9/changelog.html#airflow-1-10-9-2020-02-10)\n\nSome of the noteworthy new features (user-facing) are:\n\n- [Add tags to DAGs and use it for filtering in the UI (RBAC only)](https://github.com/apache/airflow/pull/6489)\n- [New Executor: DebugExecutor for Local debugging from your IDE](http://airflow.apache.org/docs/1.10.9/executor/debug.html)\n- [Allow passing conf in \"Add DAG Run\" (Triggered Dags) view](https://github.com/apache/airflow/pull/7281)\n- [Allow dags to run for future execution dates for manually triggered DAGs (only if `schedule_interval=None`)](https://github.com/apache/airflow/pull/7038)\n- [Dedicated page in documentation for all configs in airflow.cfg](https://airflow.apache.org/docs/1.10.9/configurations-ref.html)\n\n### Add tags to DAGs and use it for filtering in the UI\n\nIn order to filter DAGs (e.g by team), you can add tags in each dag. The filter is saved in a cookie and can be reset by the reset button.\n\nFor example:\n\nIn your Dag file, pass a list of tags you want to add to DAG object:\n\n```python\ndag = DAG(\n    dag_id='example_dag_tag',\n    schedule_interval='0 0 * * *',\n    tags=['example']\n)\n```\n\n**Screenshot**:\n![Add filter by DAG tags](airflow-dag-tags.png)\n\n**Note**: This feature is only available for the RBAC UI (enabled using `rbac=True` in `[webserver]` section in your `airflow.cfg`).\n\n\n## Special Note / Deprecations\n\n### Python 2\nPython 2 has reached end of its life on Jan 2020. Airflow Master no longer supports Python 2.\nAirflow 1.10.* would be the last series to support Python 2.\n\nWe strongly recommend users to use Python >= 3.6\n\n### Use Airflow RBAC UI\nAirflow 1.10.9 ships with 2 UIs, the default is non-RBAC Flask-admin based UI and Flask-appbuilder based UI.\n\nThe Flask-AppBuilder (FAB) based UI is allows Role-based Access Control and has more advanced features compared to\nthe legacy Flask-admin based UI. This UI can be enabled by setting `rbac=True` in `[webserver]` section in your `airflow.cfg`.\n\nFlask-admin based UI is deprecated and new features won't be ported to it. This UI will still be the default\nfor 1.10.* series but would no longer be available from Airflow 2.0\n\n\n## List of Contributors\n\nAccording to git shortlog, the f","url":"airflow-1.10.8-1.10","title":"Apache Airflow 1.10.8 & 1.10.9","linkTitle":"Apache Airflow 1.10.8 & 1.10.9","author":"Kaxil Naik","twitter":"kaxil","github":"kaxil","linkedin":"kaxil","description":"We are happy to present the new 1.10.8 and 1.10.9 releases of Apache Airflow.","tags":["release"],"date":"2020-02-23T00:00:00.000Z"},{"content":"# Apache Airflow Survey 2019\n\nApache Airflow is [growing faster than ever](https://www.astronomer.io/blog/why-airflow/).\nThus, receiving and adjusting to our users’ feedback is a must. We created\n[survey](https://forms.gle/XAzR1pQBZiftvPQM7) and we got **308** responses.\nLet’s see who Airflow users are, how they play with it, and what they miss.\n\n# Overview of the user\n\n**What best describes your current occupation?**\n\n|                         |No.|  %   |\n|-------------------------|---|------|\n|Data Engineer            |194|62.99%|\n|Developer                | 34|11.04%|\n|Architect                | 23|7.47% |\n|Data Scientist           | 19|6.17% |\n|Data Analyst             | 13|4.22% |\n|DevOps                   | 13|4.22% |\n|IT Administrator         |  2|0.65% |\n|Machine Learning Engineer|  2|0.65% |\n|Manager                  |  2|0.65% |\n|Operations               |  2|0.65% |\n|Chief Data Officer       |  1|0.32% |\n|Engineering Manager      |  1|0.32% |\n|Intern                   |  1|0.32% |\n|Product owner            |  1|0.32% |\n|Quant                    |  1|0.32% |\n\n\n**In your day to day job, what do you use Airflow for?**\n\n|                                                      |No.|  %   |\n|------------------------------------------------------|---|------|\n|Data processing (ETL)                                 |298|96.75%|\n|Artificial Intelligence and Machine Learning Pipelines| 90|29.22%|\n|Automating DevOps operations                          | 64|20.78%|\n\nAccording to the survey, most of the Airflow users are the “data” people. Moreover,\n28.57% uses Airflow to both ETL and ML pipelines meaning that those two fields\nare somehow connected. Only five respondents use Airflow for DevOps operations only,\nThat means that other 59 people who use Airflow for DevOps stuff use it also for\nETL / ML  purposes.\n\n**How many active DAGs do you have in your largest Airflow instance?**\n\n|       |No.|  %   |\n|-------|---|------|\n|0-20   |115|37.34%|\n|21-40  | 65|21.10%|\n|41-60  | 44|14.29%|\n|61-100 | 28|9.09% |\n|101-200| 28|9.09% |\n|201-300|  7|2.27% |\n|301-999|  8|2.60% |\n|1000+  | 13|4.22% |\n\n\nThe majority of users do not exceed 100 active DAGs per Airflow instance. However,\nas we can see there are users who exceed thousands of DAGs with a maximum number 5000.\n\n**What is the maximum number of tasks that you have used in one DAG?**\n\n|       |No.|  %   |\n|-------|---|------|\n|0-10   | 61|19.81%|\n|11-20  | 60|19.48%|\n|21-30  | 31|10.06%|\n|31-40  | 21|6.82% |\n|41-50  | 26|8.44% |\n|51-100 | 36|11.69%|\n|101-200| 28|9.09% |\n|201-500| 21|6.82% |\n|501+   | 24|11.54%|\n\n\nThe given maximum number of tasks in a single DAG was 10 000 (!). The number of tasks\ndepends on the purposes of a DAG, so it’s rather hard to say if users have “simple”\nor “complicated” workflows.\n\n**When onboarding new members to Airflow, what is the biggest problem?**\n\n|                                                               |No.|  %   |\n|----------------------------------------------------","url":"airflow-survey","title":"Airflow Survey 2019","linkTitle":"Airflow Survey 2019","author":"Tomek Urbaszek","twitter":"Nuclearriot","github":"nuclearpinguin","linkedin":"tomaszurbaszek","description":"Receiving and adjusting to our users’ feedback is a must. Let’s see who Airflow users are, how they play with it, and what they miss.","tags":["community","survey","users"],"date":"2019-12-11"},{"content":"I am proud to announce that Apache Airflow 2.0.0 has been released.\n\nThe full changelog is about 3,000 lines long (already excluding everything backported to 1.10), so for now I'll simply share some of the major features in 2.0.0 compared to 1.10.14:\n\n## A new way of writing dags: the TaskFlow API (AIP-31)\n\n(Known in 2.0.0alphas as Functional DAGs.)\n\nDAGs are now much much nicer to author especially when using PythonOperator. Dependencies are handled more clearly and XCom is nicer to use\n\nRead more here:\n\n[TaskFlow API Tutorial](http://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html) \\\n[TaskFlow API Documentation](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#decorated-flows)\n\nA quick teaser of what DAGs can now look like:\n\n```python\nfrom airflow.decorators import dag, task\nfrom airflow.utils.dates import days_ago\n\n@dag(default_args={'owner': 'airflow'}, schedule_interval=None, start_date=days_ago(2))\ndef tutorial_taskflow_api_etl():\n   @task\n   def extract():\n       return {\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}\n\n   @task\n   def transform(order_data_dict: dict) -> dict:\n       total_order_value = 0\n\n       for value in order_data_dict.values():\n           total_order_value += value\n\n       return {\"total_order_value\": total_order_value}\n\n   @task()\n   def load(total_order_value: float):\n\n       print(\"Total order value is: %.2f\" % total_order_value)\n\n   order_data = extract()\n   order_summary = transform(order_data)\n   load(order_summary[\"total_order_value\"])\n\ntutorial_etl_dag = tutorial_taskflow_api_etl()\n```\n\n## Fully specified REST API (AIP-32)\n\nWe now have a fully supported, no-longer-experimental API with a comprehensive OpenAPI specification\n\nRead more here:\n\n[REST API Documentation](http://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html).\n\n## Massive Scheduler performance improvements\n\nAs part of AIP-15 (Scheduler HA+performance) and other work Kamil did, we significantly improved the performance of the Airflow Scheduler. It now starts tasks much, MUCH quicker.\n\nOver at Astronomer.io we've [benchmarked the scheduler—it's fast](https://www.astronomer.io/blog/airflow-2-scheduler) (we had to triple check the numbers as we don't quite believe them at first!)\n\n## Scheduler is now HA compatible (AIP-15)\n\nIt's now possible and supported to run more than a single scheduler instance. This is super useful for both resiliency (in case a scheduler goes down) and scheduling performance.\n\nTo fully use this feature you need Postgres 9.6+ or MySQL 8+ (MySQL 5, and MariaDB won't work with more than one scheduler I'm afraid).\n\nThere's no config or other set up required to run more than one scheduler—just start up a scheduler somewhere else (ensuring it has access to the DAG files) and it will cooperate with your existing schedulers through the database.\n\nFor more information, read the [Scheduler HA documentation](http://airflow.apache.org/docs/apache-airflow/stable/scheduler.htm","url":"airflow-two-point-oh-is-here","title":"Apache Airflow 2.0 is here!","linkTitle":"Apache Airflow 2.0 is here!","author":"Ash Berlin-Taylor","github":"ashb","linkedin":"ashberlin","description":"We're proud to announce that Apache Airflow 2.0.0 has been released.","tags":["Release"],"date":"2020-12-17"},{"content":"The brand [new Airflow website](https://airflow.apache.org/) has arrived! Those who have been following the process know that the journey to update [the old Airflow website](https://airflow.readthedocs.io/en/1.10.6/) started at the beginning of the year.\nThanks to sponsorship from the Cloud Composer team at Google that allowed us to\ncollaborate with [Polidea](https://www.polidea.com/) and with their design studio [Utilo](https://utilodesign.com/), and deliver an awesome website.\n\nDocumentation of open source projects is key to engaging new contributors in the maintenance,\ndevelopment, and adoption of software. We want the Apache Airflow community to have\nthe best possible experience to contribute and use the project. We also took this opportunity to make the project\nmore accessible, and in doing so, increase its reach.\n\nIn the past three and a half months, we have updated everything: created a more efficient landing page,\nenhanced information architecture, and improved UX & UI. Most importantly, the website now has capabilities\nto be translated into many languages. This is our effort to foster a more inclusive community around\nApache Airflow, and we look forward to seeing contributions in Spanish, Chinese, Russian, and other languages as well!\n\nWe built our website on Docsy, a platform that is easy to use and contribute to. Follow\n[these steps](https://github.com/apache/airflow-site/blob/master/README.md) to set up your environment and\nto create your first pull request. You may also use\nthe new website for your own open source project as a template.\nAll of our [code is open and hosted on GitHub](https://github.com/apache/airflow-site/tree/master).\n\nShare your questions, comments, and suggestions with us, to help us improve the website.\nWe hope that this new design makes finding documentation about Airflow easier,\nand that its improved accessibility increases adoption and use of Apache Airflow around the world.\n\nHappy browsing!\n","url":"announcing-new-website","title":"New Airflow website","linkTitle":"New Airflow website","author":"Aizhamal Nurmamat kyzy","description":"We are thrilled about our new website!","tags":["Community"],"date":"2019-12-11"},{"content":"Apache Airflow is a platform to programmatically author, schedule, and monitor workflows.\nA workflow is a sequence of tasks that processes a set of data. You can think of workflow as the\npath that describes how tasks go from being undone to done. Scheduling, on the other hand, is the\nprocess of planning, controlling, and optimizing when a particular task should be done.\n\n### Authoring Workflow in Apache Airflow.\nAirflow makes it easy to author workflows using python scripts. A [Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)\n(DAG) represents a workflow in Airflow. It is a collection of tasks in a way that shows each task's\nrelationships and dependencies. You can have as many DAGs as you want, and Airflow will execute\nthem according to the task's relationships and dependencies. If task B depends on the successful\nexecution of another task A, it means Airflow will run task A and only run task B after task A.\nThis dependency is very easy to express in Airflow. For example, the above scenario is expressed as\n```python\ntask_A >> task_B\n```\nAlso equivalent to\n```python\ntask_A.set_downstream(task_B)\n```\n![Simple Dag](Simple_dag.png)\n\nThat helps Airflow to know that it needs to execute task A before task B. Tasks can have far more complex\nrelationships to each other than expressed above and Airflow figures out how and when to execute the tasks following\ntheir relationships and dependencies.\n![Complex Dag](semicomplex.png)\n\nBefore we discuss the architecture of Airflow that makes scheduling, executing, and monitoring of\nworkflow an easy thing, let us discuss the [Breeze environment](https://github.com/apache/airflow/blob/master/BREEZE.rst).\n\n### Breeze Environment\nThe breeze environment is the development environment for Airflow where you can run tests, build images,\nbuild documentations and so many other things. There are excellent\n[documentation and video](https://github.com/apache/airflow/blob/master/BREEZE.rst) on Breeze environment.\nPlease check them out. You enter the Breeze environment by running the ``./breeze`` script. You can run all\nthe commands mentioned here in the Breeze environment.\n\n### Scheduler\nThe scheduler is the component that monitors DAGs and triggers those tasks whose dependencies have\nbeen met. It watches over the DAG folder, checking the tasks in each DAG and triggers them once they\nare ready. It accomplishes this by spawning a process that runs periodically(every minute or so)\nreading the metadata database to check the status of each task and decides what needs to be done.\nThe metadata database is where the status of all tasks are recorded. The status can be one of running,\n success, failed, etc.\n\nA task is said to be ready when its dependencies have been met. The dependencies include all the data\nnecessary for the task to be executed. It should be noted that the scheduler won't trigger your tasks until\nthe period it covers has ended. If a task's ``schedule_interval`` is ``@daily``, the scheduler tr","url":"apache-airflow-for-newcomers","title":"Apache Airflow For Newcomers","linkTitle":"Apache Airflow For Newcomers","author":"Ephraim Anierobi","twitter":"ephraimbuddy","github":"ephraimbuddy","description":"","tags":["Community"],"date":"2020-08-17","draft":false},{"content":"Is it possible to create an organization that delivers tens of projects used by millions, nearly no one is paid for doing their job, and still, it has been fruitfully carrying on for more than 20 years? Apache Software Foundation proves it is possible. For the last two decades, ASF has been crafting a model called the Apache Way—a way of organizing and leading tech open source projects. Due to this approach, which is strongly based on the “community over code” motto, we can enjoy such awesome projects like Apache Spark, Flink, Beam, or Airflow (and many more).\n\nAfter this year’s ApacheCon, Polidea’s engineers talked with Committers of Apache projects, such as—Aizhamal Nurmamat kyzy, Felix Uellendall, and Fokko Driesprong—about insights to what makes the ASF such an amazing organization.\n\nYou can read the [insights after the ApacheCon 2019](https://higrys.medium.com/apachecon-europe-2019-thoughts-and-insights-by-airflow-committers-9ff5f6938c99).\n","url":"apache-con-europe-2019-thoughts-and-insights-by-airflow-committers","title":"ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers","linkTitle":"ApacheCon Europe 2019 — Thoughts and Insights by Airflow Committers","author":"Polidea","description":"Here come some thoughts by Airflow committers and contributors from the ApacheCon Europe 2019. Get to know the ASF community!","tags":["Community"],"date":"2019-11-22"},{"content":"## Documenting local development environment of Apache Airflow\n\nFrom Sept to November, 2019 I have been participating in a wonderful initiative, [Google Season of Docs](https://developers.google.com/season-of-docs).\n\nI had a pleasure to contribute to the Apache Airflow open source project as a technical writer.\nMy initial assignment was an extension to the GitHub-based Contribution guide.\n\nFrom the very first days I have been pretty closely involved into inter-project communications\nvia emails/slack and had regular 1:1s with my mentor, Jarek Potiuk.\n\nI got infected with Jarek’s enthusiasm to ease the on-boarding experience for\nAirflow contributors. I do share this strategy and did my best to improve the structure,\nlanguage and DX. As a result, Jarek and I extended the current contributor’s docs and\nended up with the Contributing guide navigating the users through the project\ninfrastructure and providing a workflow example based on a real-life use case;\nthe Testing guide with an overview of a complex testing infrastructure for Apache Airflow;\nand two guides dedicated to the Breeze dev environment and local virtual environment\n(my initial assignment).\n\nI’m deeply grateful to my mentor and Airflow developers for their feedback,\npatience and help while I was breaking through new challenges\n(I’ve never worked on an open source project before),\nand for their support of all my ideas! I think a key success factor for any contributor\nis a responsive, supportive and motivated team, and I was lucky to join such\na team for 3 months.\n\nDocuments I worked on:\n\n* [Breeze development environment documentation](https://github.com/apache/airflow/blob/master/BREEZE.rst)\n* [Local virtualenv environment documentation](https://github.com/apache/airflow/blob/master/LOCAL_VIRTUALENV.rst)\n* [Contributing guide](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)\n* [Testing guide](https://github.com/apache/airflow/blob/master/TESTING.rst)\n","url":"documenting-using-local-development-environments","title":"Documenting using local development environment","linkTitle":"Documenting using local development environment","author":"Elena Fedotova","github":"efedotova","linkedin":"elena-fedotova-039294110","description":"The story behind documenting local development environment of Apache Airflow","tags":["Development"],"date":"2019-11-22T00:00:00.000Z"},{"content":"I came across [Google Season of Docs][1] (GSoD) almost by accident, thanks to my extensive HackerNews and Twitter addiction.  I was familiar with the Google Summer of Code but not with this program.\nIt turns out it was the inaugural phase. I read the details, and the process felt a lot like GSoC except that this was about documentation.\n\n## About Me\nI have been writing tech articles on medium as well as my blog for the past 1.5 years.  Blogging helps me test my understanding of the concepts as untangling the toughest of ideas in simple sentences requires a considerable time investment.\n\nAlso, I have been working as a Software Developer for the past three years, which involves writing documentation for my projects as well. I completed my B.Tech from  IIT Roorkee. During my stay in college, I applied for GSoC once but didn’t make it through in the final list of selected candidates.\n\nI saw GSoD as an excellent opportunity to improve my technical writing skills using feedback from the open-source community. I contributed some bug fixes and features to Apache Superset and Apache Druid, but this would be my first contribution as a technical writer.\n\n## Searching for the organization\nAbout 40+ organizations were participating in the GSoD. However, there were two which came as the right choice for me in the first instant. The first one was [Apache Airflow][2] because I had already used Airflow extensively and also contributed some custom operators inside the forked version of my previous company.\n\nThe second one was [Apache Cassandra][3], on which I also had worked extensively but hadn’t done any code or doc changes.\n\nConsidering the total experience, I decided to go with the Airflow.\n\n## Project selection\nAfter selecting the org, the next step was to choose the project. Again, my previous experience played a role here, and I ended up picking the **How to create a workflow** . The aim of the project was to write documentation which will help users in creating complex as well as custom DAGs.  \nThe final deliverables were a bit different, though. More on that later.\n\nAfter submitting my application, I got involved in my job until one day, I saw a mail from google confirming my selection as a Technical Writer for the project.\n\n## Community Bonding\nGetting selected is just a beginning.  I got the invite to the Airflow slack channel where most of the discussions happened.\nMy mentor was [Ash-Berlin Taylor][4] from Apache Airflow. I started talking to my mentor to get a general sense of what deliverables were expected. The deliverables were documented in [confluence][5].\n\n- A page for how to create a DAG that also includes:\n    - Revamping the page related to scheduling a DAG\n    - Adding tips for specific DAG conditions, such as rerunning a failed task\n- A page for developing custom operators that includes:\n    - Describing mechanisms that are important when creating an operator, such as template fields, UI color, hooks, connection, etc.\n    - Describing the r","url":"experience-in-google-season-of-docs-2019-with-apache-airflow","title":"Experience in Google Season of Docs 2019 with Apache Airflow","linkTitle":"Experience in Google Season of Docs 2019 with Apache Airflow","author":"Kartik Khare","twitter":"khare_khote","github":"KKcorps","linkedin":"kharekartik","description":"","tags":["Documentation"],"date":"2019-12-20T00:00:00.000Z"},{"content":"[Outreachy](https://www.outreachy.org/) is a program which organises three months paid internships with FOSS\nprojects for people who are typically underrepresented in those projects.\n\n### Contribution Period\nThe first thing I had to do was choose a project under an organisation. After going through all the projects\nI chose “Extending the REST API of Apache Airflow”, because I had a good idea of what  REST API(s) are, so I\nthought it would be easier to get started with the contributions. The next step was to set up Airflow’s dev\nenvironment which thanks to [Breeze](https://github.com/apache/airflow/blob/master/BREEZE.rst), was a breeze.\nSince I had never contributed to FOSS before so this part was overwhelming but there were plenty of issues\nlabelled “good first issues” with detailed descriptions and some even had code snippets so luckily that nudged\nme in the right direction. These things about Airflow and the positive vibes from the community were the reasons\nwhy I chose to stick with Airflow as my Outreachy project.\n\n### Internship Period\nMy first PR was followed by many new experiences one of them being that I introduced a\n[bug](https://github.com/apache/airflow/pull/7680#issuecomment-619763051) in it;).\nBut nonetheless it made me familiar with the feedback loop and the feedback on my subsequent\n[PRs](https://github.com/apache/airflow/pulls?q=is%3Apr+author%3AOmairK+) was the focal point of the overall\nlearning experience I went through, which boosted my confidence to contribute more and move out of my comfort zone.\nI wanted to learn more about the things that happen under the Airflow’s hood so I started filtering out recent PRs\ndealing with different components and I would go through the code changes along with discussion that would help me\nget a better understanding of the whole workflow. [Airflow’s mailing list](https://lists.apache.org/list.html?dev@airflow.apache.org)\nwas also a great source of knowledge.\n\nThe API related PRs that I worked on helped me with some of the important concepts like:\n\n  1) [Pool CRUD endpoints](https://github.com/apache/airflow/pull/9329) where pools limit the execution parallelism.\n\n  2) [Tasks](https://github.com/apache/airflow/pull/9597) determine the actual work that has to be carried out.\n\n  3) [DAG](https://github.com/apache/airflow/pull/9473) which represents the structure for a collection\n  of tasks. It keeps track of tasks, their dependencies and the sequence in which they have to run.\n\n  4) [Dag Runs](https://github.com/apache/airflow/pull/9473) that are the instantiation of DAG(s) in time.\n\nThrough actively and passively participating in discussions I learnt that even if there is a difference of opinion\none could always learn from the different approaches, and [this PR](https://github.com/apache/airflow/pull/8721) with\nmore than 300+ comments is the proof of it. I also started reviewing small PRs which gave me the amazing opportunity\nto interact with new people. Throughout my internship I learnt a lot","url":"experience-with-airflow-as-an-outreachy-intern","title":"Journey with Airflow as an Outreachy Intern","linkTitle":"Journey with Airflow as an Outreachy Intern","author":"Omair Khan","github":"OmairK","linkedin":"omairkhan64","description":"","tags":["Community"],"date":"2020-08-30"},{"content":"My [Outreachy internship](https://outreachy.org) is coming to its ends which is also the best time to look back and\nreflect on the progress so far.\n\nThe goal of my project is to Extend and Improve the Apache Airflow REST API. In this post,\nI will be sharing my progress so far.\n\nWe started a bit late implementing the REST API because it took time for the OpenAPI 3.0\nspecification we were to use for the project to be merged. Thanks to [Kamil](https://github.com/mik-laj),\nwho paved the way for us to start implementing the REST API endpoints. Below are the endpoints I\nimplemented and the challenges I encountered, including how I overcame them.\n\n### Implementing The Read-Only Connection Endpoints\nThe [read-only connection endpoints](https://github.com/apache/airflow/pull/9095) were the first endpoint I implemented. Looking back,\nI can see how much I have improved.\n\nI started by implementing the database schema for the Connection table using [Marshmallow 2](https://marshmallow.readthedocs.io/en/2.x-line/).\nWe had to use Marshmallow 2 because Flask-AppBuilder was still using it and Flask-AppBuilder\nis deeply integrated to Apache Airflow. This meant I had to unlearn Marshmallow 3 that I had\n been studying before this realization, but thankfully, [Marshmallow 3](https://marshmallow.readthedocs.io/en/stable/index.html) isn't too\n different, so I was able to start using Marshmallow 2 in no time.\n\nThis first PR would have been more difficult than it was unless there had been any reference\nendpoint to look at. [Kamil](https://github.com/mik-laj) implemented a [draft PR](https://github.com/apache/airflow/pull/9045) in which I took inspiration from.\nThanks to this, It was easy for me to write the unit tests. It was also in this endpoint that\n I learned using [parameterized](https://github.com/wolever/parameterized) in unit tests :D.\n\n### Implementing The Read-Only DagRuns Endpoints\n\nThis [endpoint](https://github.com/apache/airflow/pull/9153) came with its many challenges, especially on filtering with `datetimes`.\nThis was because the `connexion` library we were using to build the REST API was not validating\ndate-time format in OpenAPI 3.0 specification, what I eventually found out, was intentional.\nConnexion dropped `strict-rfc3339` because of the later license which is not compatible with\nApache 2.0 license.\n\nI implemented a workaround on this, by defining a function called `conn_parse_datetime` in the\nAPI utils module. This was later refactored and thankfully, [Kamil](https://github.com/mik-laj)\n implemented a decorator that allowed us to have cleaner code on the views while using this function.\n\nThen we tried using `rfc3339-validator` whose license is compatible with Apache 2.0 licence but\n later discarded this because with our custom date parser we were able to use duration and\n not just date times.\n\n### Other Endpoints\nI implemented some different other endpoints. One peculiar issue I faced was because of Marshmallow 2\nnot giving error when extra fields a","url":"implementing-stable-API-for-Apache-Airflow","title":"Implementing Stable API for Apache Airflow","linkTitle":"Implementing Stable API for Apache Airflow","author":"Ephraim Anierobi","twitter":"ephraimbuddy","github":"ephraimbuddy","description":"An Outreachy intern's progress report on contributing to Apache Airflow REST API.","tags":["REST API"],"date":"2020-07-19"}]